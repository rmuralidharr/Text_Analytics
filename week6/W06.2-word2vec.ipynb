{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Word embeddings are vector representations of words that capture their semantic meaning. Word embeddings are often used as a fundamental component for downstream natural language processing (NLP) tasks, such as machine translation, sentiment analysis, and text summarization. Word embeddings are typically generated by training a shallow neural network on a large corpus of text. The resulting word embeddings can then be used to perform various NLP tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Word2Vec\n",
    "\n",
    "Word2Vec is a popular algorithm for generating word embeddings, which are vector representations of words that capture their semantic meaning. Gensim is a Python library that provides an easy-to-use interface for training and using Word2Vec models. With Gensim, you can train Word2Vec models on your own text data, or use pre-trained models to perform various natural language processing tasks. In this introduction, we'll explore how to use Gensim to train Word2Vec models on your own text data.\n",
    "\n",
    "Though there are many implimentations, gensim is one of the more popular ones. It is also one of the more efficient ones.\n",
    "\n",
    "https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html?highlight=king%20man\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Word2Vec\n",
    "\n",
    "Word2Vec is an algorithm. In order to use this algorithm to generate word embeddings, we need a trained version.\n",
    "\n",
    "1) You can train your own model. To create a model you will need to provide it with a large corpus of text. The Word2Vec algorithm will then generate a vector representation for each word in the corpus. The resulting word embeddings can then be used to perform various NLP tasks.\n",
    "2) Use a pretrained Word2Vec model. There are many pretrained Word2Vec models available online. These models have been trained on large corpuses of text, and can be used to perform various NLP tasks. \n",
    "3) Use a pretrained Word2Vec model and fine-tune it on your own corpus of text. This is a good option if you have a small corpus of text, and want to improve the performance of the pretrained model on your own data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Train your own Word2Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a small 'toy example' of how to train your own Word2Vec model using Gensim. In this example, we'll train a Word2Vec model on a small corpus of text. The resulting word embeddings can then be used to perform various NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.16 ms, sys: 1.77 ms, total: 3.93 ms\n",
      "Wall time: 2.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define a list of sentences to train on\n",
    "sentences = [[\"I\", \"love\", \"chocolate\"],\n",
    "             [\"Chocolate\", \"is\", \"my\", \"favorite\", \"food\"],\n",
    "             [\"I\", \"love\", \"ice\", \"cream\"],\n",
    "             [\"Ice\", \"cream\", \"is\", \"delicious\"]]\n",
    "\n",
    "# Train a Word2Vec model on the sentences\n",
    "model = Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([ 9.7702928e-03,  8.1651136e-03,  1.2809718e-03,  5.0975787e-03,\n",
      "        1.4081288e-03, -6.4551616e-03, -1.4280510e-03,  6.4491653e-03,\n",
      "       -4.6173059e-03, -3.9930656e-03,  4.9244044e-03,  2.7130984e-03,\n",
      "       -1.8479753e-03, -2.8769434e-03,  6.0107317e-03, -5.7167388e-03,\n",
      "       -3.2367026e-03, -6.4878250e-03, -4.2346325e-03, -8.5809948e-03,\n",
      "       -4.4697891e-03, -8.5112294e-03,  1.4037776e-03, -8.6181965e-03,\n",
      "       -9.9166557e-03, -8.2016252e-03, -6.7726658e-03,  6.6805850e-03,\n",
      "        3.7845564e-03,  3.5616636e-04, -2.9579818e-03, -7.4283206e-03,\n",
      "        5.3341867e-04,  4.9989222e-04,  1.9561886e-04,  8.5259555e-04,\n",
      "        7.8633073e-04, -6.8160298e-05, -8.0070542e-03, -5.8702733e-03,\n",
      "       -8.3829118e-03, -1.3120425e-03,  1.8206370e-03,  7.4171280e-03,\n",
      "       -1.9634271e-03, -2.3252917e-03,  9.4871549e-03,  7.9704521e-05,\n",
      "       -2.4045217e-03,  8.6048469e-03,  2.6870037e-03, -5.3439722e-03,\n",
      "        6.5881060e-03,  4.5101536e-03, -7.0544672e-03, -3.2317400e-04,\n",
      "        8.3448651e-04,  5.7473574e-03, -1.7176545e-03, -2.8065301e-03,\n",
      "        1.7484308e-03,  8.4717153e-04,  1.1928272e-03, -2.6342822e-03,\n",
      "       -5.9857843e-03,  7.3229838e-03,  7.5873756e-03,  8.2963575e-03,\n",
      "       -8.5988473e-03,  2.6364254e-03, -3.5599626e-03,  9.6204039e-03,\n",
      "        2.9037679e-03,  4.6411133e-03,  2.3856151e-03,  6.6084778e-03,\n",
      "       -5.7432903e-03,  7.8944126e-03, -2.4109220e-03, -4.5618857e-03,\n",
      "       -2.0609903e-03,  9.7335577e-03, -6.8565905e-03, -2.1917201e-03,\n",
      "        7.0009995e-03, -5.5749417e-05, -6.2949671e-03, -6.3935257e-03,\n",
      "        8.9403950e-03,  6.4295758e-03,  4.7735930e-03, -3.2620477e-03,\n",
      "       -9.2676198e-03,  3.7868882e-03,  7.1605504e-03, -5.6328895e-03,\n",
      "       -7.8650126e-03, -2.9727400e-03, -4.9318983e-03, -2.3151112e-03],\n",
      "      dtype=float32)\n",
      "CPU times: user 475 µs, sys: 71 µs, total: 546 µs\n",
      "Wall time: 557 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Get the vector representation of a word\n",
    "vector = model.wv[\"chocolate\"]\n",
    "\n",
    "pprint(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('love', 0.17272792756557465),\n",
      " ('Ice', 0.16694681346416473),\n",
      " ('food', 0.11117953062057495),\n",
      " ('Chocolate', 0.10941851884126663),\n",
      " ('cream', 0.07963486015796661),\n",
      " ('I', 0.04130808636546135),\n",
      " ('favorite', 0.03771297261118889),\n",
      " ('is', 0.008315940387547016),\n",
      " ('delicious', -0.005896823015064001),\n",
      " ('my', -0.07424271106719971)]\n"
     ]
    }
   ],
   "source": [
    "# Find the most similar words to a given word\n",
    "similar_words = model.wv.most_similar(\"chocolate\")\n",
    "\n",
    "pprint(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('my', -0.07424271106719971),\n",
      " ('delicious', -0.005896823015064001),\n",
      " ('is', 0.008315940387547016),\n",
      " ('favorite', 0.03771297261118889),\n",
      " ('I', 0.04130808636546135),\n",
      " ('cream', 0.07963486015796661),\n",
      " ('Chocolate', 0.10941851884126663),\n",
      " ('food', 0.11117953062057495),\n",
      " ('Ice', 0.16694681346416473),\n",
      " ('love', 0.17272792756557465)]\n"
     ]
    }
   ],
   "source": [
    "sorted_array = sorted(similar_words, key=lambda x: x[1])\n",
    "\n",
    "pprint(sorted_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a Pretrained Word2Vec Model\n",
    "\n",
    "In this example we will use Google's word2vec-google-news-300 model. This model contains 300-dimensional vectors for 3 million words and phrases. You can download the model directly from the original Word2Vec authors github repo:\n",
    "* https://github.com/mmihaltz/word2vec-GoogleNews-vectors/blob/master/GoogleNews-vectors-negative300.bin.gz\n",
    "  \n",
    "NOTE: You can also use the GenSim downloader to download the model. However, the download can be slow and may take longer than downloading the file from github (directly from the original authors repo)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.8 s, sys: 890 ms, total: 21.7 s\n",
      "Wall time: 21.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load a pre-trained Word2Vec model\n",
    "model_path = \"./data/GoogleNews-vectors-negative300.bin.gz\" # change ./data if you do not have the data folder in the same directory as this file\n",
    "model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "\n",
    "# or, as discussed above...\n",
    "#import gensim.downloader as api\n",
    "#model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([ 1.57226562e-01, -7.08007812e-02,  5.39550781e-02, -1.89208984e-02,\n",
      "        9.17968750e-02,  2.55126953e-02,  7.37304688e-02, -5.68847656e-02,\n",
      "        1.79687500e-01,  9.27734375e-02,  9.03320312e-02, -4.12109375e-01,\n",
      "       -8.30078125e-02, -1.45507812e-01, -2.37304688e-01, -3.68652344e-02,\n",
      "        8.74023438e-02, -2.77099609e-02,  1.13677979e-03,  8.30078125e-02,\n",
      "        3.57421875e-01, -2.61718750e-01,  7.47070312e-02, -8.10546875e-02,\n",
      "       -2.35595703e-02, -1.61132812e-01, -4.78515625e-02,  1.85546875e-01,\n",
      "       -3.97949219e-02, -1.58203125e-01, -4.37011719e-02, -1.11328125e-01,\n",
      "       -1.05957031e-01,  9.86328125e-02, -8.34960938e-02, -1.27929688e-01,\n",
      "       -1.39648438e-01, -1.86523438e-01, -5.71289062e-02, -1.17675781e-01,\n",
      "       -1.32812500e-01,  1.55639648e-02,  1.34765625e-01,  8.39843750e-02,\n",
      "       -9.03320312e-02, -4.12597656e-02, -2.51953125e-01, -2.27539062e-01,\n",
      "       -6.64062500e-02, -7.66601562e-02,  5.15136719e-02,  5.90820312e-02,\n",
      "        3.49609375e-01, -1.13769531e-01, -2.57568359e-02, -1.98242188e-01,\n",
      "        4.44335938e-02,  1.09863281e-01,  1.04003906e-01, -1.75781250e-01,\n",
      "        1.22558594e-01,  7.81250000e-02,  6.20117188e-02,  6.49414062e-02,\n",
      "       -1.73828125e-01, -1.11694336e-02,  1.88476562e-01,  3.34472656e-02,\n",
      "       -4.29687500e-02, -4.71191406e-02,  2.91015625e-01,  4.19921875e-02,\n",
      "        1.59179688e-01,  1.22558594e-01, -2.55859375e-01,  2.44140625e-01,\n",
      "        1.54296875e-01, -3.46679688e-02,  1.24023438e-01, -1.32812500e-01,\n",
      "        8.44726562e-02,  3.71093750e-02, -1.05468750e-01,  9.81445312e-02,\n",
      "       -8.23974609e-03,  5.34667969e-02, -1.96838379e-03,  9.03320312e-02,\n",
      "        1.30859375e-01, -1.57470703e-02, -2.40478516e-02, -3.29589844e-02,\n",
      "       -5.63964844e-02, -3.12500000e-01, -1.19140625e-01,  4.41894531e-02,\n",
      "       -1.82617188e-01, -2.20703125e-01,  8.39843750e-02, -2.15820312e-01,\n",
      "       -1.60156250e-01, -2.01171875e-01,  1.63085938e-01, -4.57763672e-05,\n",
      "        4.24804688e-02, -1.37695312e-01, -2.62451172e-02,  1.53320312e-01,\n",
      "       -1.07421875e-01, -1.34765625e-01, -3.73840332e-03, -1.51977539e-02,\n",
      "       -7.27539062e-02,  3.22265625e-02,  1.89453125e-01, -8.00781250e-02,\n",
      "        1.45507812e-01, -9.66796875e-02, -9.27734375e-02,  8.91113281e-03,\n",
      "       -4.27246094e-02, -9.76562500e-02,  3.29589844e-02, -7.95898438e-02,\n",
      "       -6.25000000e-02,  3.39355469e-02,  1.05590820e-02, -1.28906250e-01,\n",
      "        1.09863281e-01,  1.89453125e-01,  1.52343750e-01, -1.47460938e-01,\n",
      "       -3.86047363e-03,  1.75781250e-01, -4.58984375e-02, -1.02539062e-01,\n",
      "        6.34765625e-02, -9.86328125e-02,  1.87500000e-01,  3.97949219e-02,\n",
      "       -2.65625000e-01, -1.24023438e-01, -1.35742188e-01,  7.93457031e-03,\n",
      "        6.59179688e-02,  8.11767578e-03, -3.24707031e-02, -1.03759766e-02,\n",
      "       -1.90429688e-02, -8.20312500e-02,  2.06054688e-01,  1.40625000e-01,\n",
      "        1.93359375e-01,  2.91015625e-01, -9.17968750e-02, -1.40625000e-01,\n",
      "       -1.75781250e-01, -1.36718750e-01,  2.51464844e-02, -5.83496094e-02,\n",
      "       -1.84570312e-01,  3.10546875e-01,  7.17773438e-02, -1.01074219e-01,\n",
      "        1.08886719e-01, -2.23388672e-02,  1.50390625e-01, -7.03125000e-02,\n",
      "        1.24023438e-01,  2.21679688e-01, -1.97265625e-01, -6.05468750e-02,\n",
      "       -4.30297852e-03, -1.69921875e-01, -1.45507812e-01, -2.17773438e-01,\n",
      "        2.47070312e-01,  6.64062500e-02, -8.05664062e-02,  3.57421875e-01,\n",
      "       -8.20312500e-02, -7.87353516e-03,  1.08886719e-01, -5.32226562e-02,\n",
      "        3.00781250e-01, -2.37304688e-01,  1.61132812e-01,  1.59179688e-01,\n",
      "        1.69921875e-01, -9.52148438e-02,  5.20019531e-02, -6.22558594e-02,\n",
      "       -8.85009766e-03,  4.68750000e-02, -2.88085938e-02,  1.25000000e-01,\n",
      "        3.49121094e-02,  4.61425781e-02,  1.66015625e-02, -9.57031250e-02,\n",
      "       -1.48437500e-01, -1.64794922e-02, -2.22656250e-01, -2.51953125e-01,\n",
      "       -3.58886719e-02, -2.52685547e-02,  8.39233398e-05,  6.98242188e-02,\n",
      "        2.53906250e-01, -3.29589844e-02,  6.59179688e-02,  6.28662109e-03,\n",
      "       -7.86132812e-02, -3.01513672e-02, -9.47265625e-02,  1.25000000e-01,\n",
      "       -1.62109375e-01,  2.53906250e-01, -3.30078125e-01,  6.44531250e-02,\n",
      "       -9.09423828e-03,  7.12890625e-02,  3.99780273e-03, -4.41894531e-02,\n",
      "       -1.42822266e-02, -9.52148438e-03,  1.17675781e-01,  3.49609375e-01,\n",
      "       -2.90527344e-02,  1.86767578e-02,  3.46679688e-02,  1.89208984e-02,\n",
      "       -1.26953125e-01,  2.68554688e-02, -1.06933594e-01,  1.20117188e-01,\n",
      "       -2.69775391e-02, -5.07812500e-02, -1.76757812e-01,  3.95507812e-02,\n",
      "        1.35742188e-01, -9.61914062e-02, -1.98242188e-01, -1.86767578e-02,\n",
      "       -2.47802734e-02, -5.32226562e-02,  1.54296875e-01,  5.95703125e-02,\n",
      "        6.39648438e-02, -6.17675781e-02,  3.36914062e-02,  1.75781250e-01,\n",
      "        6.59179688e-02,  2.22656250e-01, -1.28906250e-01,  4.61425781e-02,\n",
      "       -2.57812500e-01,  6.78710938e-02,  6.29882812e-02, -1.15722656e-01,\n",
      "       -2.13867188e-01, -2.53906250e-01,  2.73437500e-02, -4.68750000e-02,\n",
      "        1.38671875e-01,  2.59765625e-01, -2.07031250e-01, -9.64355469e-03,\n",
      "       -5.22460938e-02, -7.20214844e-03,  8.49609375e-02, -2.49023438e-02,\n",
      "        1.94335938e-01, -7.37304688e-02,  1.22070312e-01, -3.49121094e-02,\n",
      "        1.41601562e-01, -1.38671875e-01,  7.61718750e-02, -1.93359375e-01,\n",
      "        1.64062500e-01, -2.78320312e-02, -1.45263672e-02,  1.44531250e-01,\n",
      "        1.75781250e-01, -1.70898438e-02,  1.26953125e-01,  3.39355469e-02,\n",
      "       -2.80761719e-02,  1.82617188e-01, -2.94921875e-01,  3.78417969e-02,\n",
      "       -1.63085938e-01,  1.73828125e-01, -1.01074219e-01, -1.49414062e-01,\n",
      "       -4.17480469e-02,  9.82666016e-03, -4.94384766e-03, -3.29589844e-02],\n",
      "      dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Get the vector representation of a word\n",
    "vector = model[\"house\"]\n",
    "\n",
    "pprint(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('houses', 0.7072390913963318),\n",
      " ('bungalow', 0.6878558993339539),\n",
      " ('apartment', 0.6628996729850769),\n",
      " ('bedroom', 0.6496937274932861),\n",
      " ('townhouse', 0.6384080052375793),\n",
      " ('residence', 0.6198420524597168),\n",
      " ('mansion', 0.6058192253112793),\n",
      " ('farmhouse', 0.5857570171356201),\n",
      " ('duplex', 0.5757936835289001),\n",
      " ('appartment', 0.5690325498580933)]\n"
     ]
    }
   ],
   "source": [
    "# Find the most similar words to a given word\n",
    "similar_words = model.most_similar(\"house\")\n",
    "\n",
    "pprint(similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning an existing pre-trained model\n",
    "\n",
    "The third option we have is to fine-tune an existing pre-trained model on our own corpus of text. This is a good option if you have a small corpus of text, and want to improve the performance of the pretrained model on your own data.\n",
    "\n",
    "NOTE: This is 'tricky' stuff. You need to be careful when doing this. If you have a small corpus of text, you may not have enough data to fine-tune the model. In this case, you may end up overfitting the model to your data. If you have a large corpus of text, you may be able to fine-tune the model without overfitting. However, you may not be able to improve the performance of the model on your own data. In this case, you may be better off using the pretrained model as-is. I've included this option for completeness, but I don't recommend using it unless you have a good reason to do so (and have much more exprerience than you do now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_examples: 16\n",
      "Total epochs to be used in training: 5\n",
      "CPU times: user 28.7 s, sys: 624 ms, total: 29.3 s\n",
      "Wall time: 29.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# fine tune this model using sample sentences\n",
    "new_sentences = [\n",
    "    [\"The\", \"p220\", \"laser\", \"printer\", \"can\", \"print\", \"15\", \"pages\", \"per\", \"minute\"],\n",
    "    [\"The\", \"p220\", \"laser\", \"printer\", \"is\", \"a\", \"black\", \"and\", \"white\", \"printer\"],\n",
    "    [\"The\", \"p320\", \"laser\", \"printer\", \"is\", \"a\", \"color\", \"printer\"],\n",
    "    [\"The\", \"p320\", \"laser\", \"printer\", \"can\", \"print\", \"21\", \"pages\", \"per\", \"minute\"],\n",
    "    [\"The\", \"p220\", \"laser\", \"printer\", \"is\", \"a\", \"fast\", \"printer\"],\n",
    "    [\"The\", \"p320\", \"laser\", \"printer\", \"is\", \"a\", \"faster\", \"printer\"],\n",
    "    [\"The\", \"p220\", \"laser\", \"printer\", \"is\", \"a\", \"cheap\", \"printer\"],\n",
    "    [\"The\", \"p320\", \"laser\", \"printer\", \"is\", \"an\", \"expensive\", \"printer\"],\n",
    "    [\"The\", \"p220\", \"laser\", \"printer\", \"is\", \"a\", \"reliable\", \"printer\"],\n",
    "    [\"The\", \"p320\", \"laser\", \"printer\", \"is\", \"a\", \"reliable\", \"printer\"],\n",
    "    [\"The\", \"p220\", \"laser\", \"printer\", \"is\", \"a\", \"small\", \"printer\"],\n",
    "    [\"The\", \"p320\", \"laser\", \"printer\", \"is\", \"a\", \"large\", \"printer\"],\n",
    "    [\"The\", \"p220\", \"laser\", \"printer\", \"is\", \"a\", \"light\", \"printer\"],\n",
    "    [\"The\", \"p320\", \"laser\", \"printer\", \"is\", \"a\", \"heavy\", \"printer\"],\n",
    "    [\"The\", \"p220\", \"laser\", \"printer\", \"is\", \"a\", \"loud\", \"printer\"],\n",
    "    [\"The\", \"p320\", \"laser\", \"printer\", \"is\", \"a\", \"quiet\", \"printer\"],\n",
    "]\n",
    "\n",
    "#model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "model = Word2Vec(sentences=new_sentences, vector_size=300, window=5, min_count=1, workers=4)\n",
    "#model.build_vocab(sentences)\n",
    "\n",
    "total_examples = model.corpus_count\n",
    "print('total_examples:', total_examples)\n",
    "\n",
    "print('Total epochs to be used in training:', model.epochs)\n",
    "model.train(sentences, total_examples=total_examples, epochs=model.epochs) # model.epochs = 5 by default, but you can change it. Increasing this value will increase the training time and may improve the model's performance.\n",
    "\n",
    "model.build_vocab(new_sentences) # this is needed to update the model's vocabulary with the new sentences (for instance, p220 and p230 printer names in our example)\n",
    "model.wv.vectors_lockf = np.ones(len(model.wv)) # np.ones() is a function that returns an array of 1s with the same shape as the input array, this is needed by the new training method in gensim 4+\n",
    "model.wv.intersect_word2vec_format(\"./data/GoogleNews-vectors-negative300.bin.gz\", lockf=0.0, binary=True) # lockf = 1.0 allows for futher training, lockf = 0.0 does not\n",
    "\n",
    "model.save(\"./data/fine-tuned_word2vec_model.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('faster', 0.10351143032312393),\n",
      " ('p320', 0.0540100522339344),\n",
      " ('and', 0.05245136469602585),\n",
      " ('fast', 0.03871935233473778),\n",
      " ('white', 0.03503745049238205),\n",
      " ('black', 0.02766772173345089),\n",
      " ('pages', 0.012000232003629208),\n",
      " ('21', 0.002772286534309387),\n",
      " ('minute', -0.014604938216507435),\n",
      " ('printer', -0.018044397234916687)]\n"
     ]
    }
   ],
   "source": [
    "# Find the most similar words to a given word\n",
    "similar_words = model.wv.most_similar(\"p220\")\n",
    "\n",
    "pprint(similar_words)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
