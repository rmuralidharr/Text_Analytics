{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy\n",
    "\n",
    "Scrapy is a Python framework for large scale web scraping. It gives you all the tools you need to efficiently extract data from websites, process them as you want, and store them in your preferred structure and format. It's used for a wide range of data mining applications, from data processing to historical archival to data mining to text mining to information retrieval.\n",
    "\n",
    "You can find the documentation here: http://doc.scrapy.org/en/latest/\n",
    "\n",
    "NOTE: You may find scrapy easier to work with.\n",
    "\n",
    "In summary, we have three general approaches to web scraping:\n",
    "* use requests and BeautifulSoup\n",
    "* use requests and regex\n",
    "* use scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapy Project Tutorial\n",
    "\n",
    "1. Create a new Scrapy project.\n",
    "\n",
    "Open a terminal and run the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "scrapy startproject tutorial\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a Scrapy Spider\n",
    "\n",
    "Create a new Python file called `quotes_spider.py` inside the `spiders` folder and populate it with the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from pathlib import Path\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            \"https://quotes.toscrape.com/page/1/\",\n",
    "            \"https://quotes.toscrape.com/page/2/\",\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = f\"quotes-{page}.html\"\n",
    "        Path(filename).write_bytes(response.body)\n",
    "        self.log(f\"Saved file {filename}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Run the spider \n",
    "\n",
    "Go to your terminal. Make sure you are in the top folder of the scrapy project (in this case, tutorial) and then run the following command:\n",
    "    \n",
    "    scrapy crawl quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on scrapy, see the [Scrapy Tutorial](https://docs.scrapy.org/en/latest/intro/tutorial.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
